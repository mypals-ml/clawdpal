# Instrument: OpenClaw Mac Deployment Plan

## 1. Host Prerequisites (macOS)
Run these services directly on your Mac terminal (not in Docker) to utilize hardware/files easily.

### A. Ollama (AI Backend)
1.  **Install**: Download from [ollama.com](https://ollama.com).
2.  **Model**: Pull a vision-capable model:
    ```bash
    ollama pull llava
    ```
3.  **Serve**: Ensure it accepts external connections.
    *Why?* By default, Ollama only listens to `localhost`. Docker containers run in a separate network, so we must bind to `0.0.0.0` to allow the container to reach the host.
    ```bash
    OLLAMA_HOST=0.0.0.0 ollama serve
    ```

### B. Local File Server (The "World")
1.  **Prepare Folder**: Create a folder for test images.
    ```bash
    mkdir -p ~/clawdpal
    ```
2.  **Add Data**: Save a test image named `test.jpg` inside that folder.
3.  **Start Server**:
    ```bash
    cd ~/clawdpal
    python3 -m http.server 8000
    ```
4.  **Verify**: Open `http://localhost:8000/test.jpg` in your browser.

## 2. Docker Setup (The Agent)
We will use a dedicated folder and a `docker-compose.yml` file to manage the `clawdpal` instance.

### Step 1: Prepare Directory
```bash
mkdir -p ~/openclaw-docker/data
cd ~/openclaw-docker
```

### Step 2: Create Docker Compose File
Create a file named `docker-compose.yml` with the following content:

```yaml
services:
  clawdpal:
    container_name: clawdpal
    image: openclaw/openclaw:latest
    restart: unless-stopped
    ports:
      - "18789:18789" # Expose Gateway Port for Web UI/Apps
    volumes:
      - ./data:/app/data
    environment:
      - LLM_PROVIDER=ollama
      - LLM_BASE_URL=http://host.docker.internal:11434
      - LLM_MODEL=llava
      - PLATFORM=telegram
      - TELEGRAM_TOKEN=${TELEGRAM_TOKEN}
    extra_hosts:
      - "host.docker.internal:host-gateway"
```

### Step 3: Configure Environment
Create an `.env` file in the same directory to store your Telegram token safely:
```bash
# Create the file (you will edit this in the next section)
touch .env
```

## 3. Interface Configuration
- **Telegram Bot Setup**:
    1. Open Telegram and message `@BotFather`.
    2. Send command `/newbot`.
    3. Name it (e.g., `MyClawdpalBot`).
    4. **Copy the API Token** provided.
    5. Edit your `.env` file to add the token:
       ```bash
       echo "TELEGRAM_TOKEN=123456:ABC-DEF1234ghIkl-zyx57W2v1u123ew11" > .env
       ```

## 4. Execution & Validation
1.  **Launch Agent**:
    ```bash
    cd ~/openclaw-docker
    docker-compose up -d
    ```
    *Note: If you change the TELEGRAM_TOKEN later in `.env`, just run this command again. The agent will restart, but thanks to the `./data` volume, it will remember its history.*

2.  **Monitor Logs**:
    ```bash
    docker compose logs -f
    ```
    *Look for: "Connected to Ollama", "Telegram Polling Started".*

3.  **The "Vision Test"**:
    - **Find your Bot**: Search for `@MyClawdpalBot` (or your chosen username) in Telegram and click **Start**.
    - Send the command:
      > *"Please look at the image at http://host.docker.internal:8000/test.jpg and describe it."*
    - **Note**: We use `host.docker.internal` because the Agent is inside Docker, trying to reach port 8000 on your Mac.

## 5. Success Criteria
- Agent acknowledges the URL.
- Agent downloads the image successfully.
- Agent replies with: "This is a picture of..." (describing your test image).

## Appendix A: Switching LLM Providers

To switch providers, you may edit the files and run `docker-compose up -d` to restart (your data is safe).

**1. Anthropic (Claude)**
*   Update `.env`: Add `ANTHROPIC_API_KEY=sk-ant-...`
*   Update `docker-compose.yml`:
    ```yaml
    environment:
      - LLM_PROVIDER=anthropic
      - LLM_MODEL=claude-3-5-sonnet-20240620
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      # Remove LLM_BASE_URL for standard cloud providers
    ```

**2. Google (Gemini)**
*   Update `.env`: Add `GOOGLE_API_KEY=AIza...`
*   Update `docker-compose.yml`:
    ```yaml
    environment:
      - LLM_PROVIDER=google
      - LLM_MODEL=gemini-1.5-pro
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
    ```

**3. xAI (Grok) / OpenAI Compatible**
*   Update `.env`: Add `XAI_API_KEY=xai...`
*   Update `docker-compose.yml` (Use OpenAI provider with custom Base URL):
    ```yaml
    environment:
      - LLM_PROVIDER=openai
      - LLM_BASE_URL=https://api.x.ai/v1
      - LLM_MODEL=grok-beta
      - OPENAI_API_KEY=${XAI_API_KEY}
    ```

## Appendix B: Chat Commands
These commands work in Telegram/WhatsApp:
*   `/status` — Check model, tokens, and cost.
*   `/new` or `/reset` — Start a fresh session context.
*   `/compact` — Summarize history to save tokens.
*   `/think <level>` — Set reasoning level (off, low, medium, high).
*   `/usage` — Toggle token usage stats in footer.
*   `/restart` — Restart the gateway process.

## Appendix C: Key Concepts

### Why `OLLAMA_HOST=0.0.0.0`?
By default, Ollama is "shy"—it only listens to `localhost` (127.0.0.1). This means only programs running directly on your Mac can talk to it.
*   **Without `0.0.0.0`**: Docker tries to connect, but Ollama ignores it because the request isn't coming from `localhost`.
*   **With `0.0.0.0`**: You tell Ollama to listen to **all** network interfaces. It allows the Docker container (via the `host.docker.internal` bridge) to reach the Ollama API.

### Data Persistence
You can change the `TELEGRAM_TOKEN` or update the Docker image without losing data.
*   **Mechanism**: The line `volumes: - ./data:/app/data` maps a folder on your Mac (`~/openclaw-docker/data`) to the container's storage.
*   **Result**: If you delete the container, the data remains on your Mac. A new container will simply "plug in" to the existing data folder.
